{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization in Artificial Neural Networks\n",
    "\n",
    "Batch Normalization is a technique used in artificial neural networks to improve the training and performance of deep models. It addresses the problem of internal covariate shift, where the distribution of input values to each layer of a neural network changes during training, making it difficult for the network to converge efficiently. Batch Normalization helps mitigate this issue by normalizing the inputs of each layer.\n",
    "\n",
    "## Benefits of Using Batch Normalization\n",
    "\n",
    "Using Batch Normalization during training offers several benefits:\n",
    "\n",
    "1. **Faster Convergence**: Normalizing inputs reduces the vanishing and exploding gradient problems, allowing for faster convergence during training.\n",
    "\n",
    "2. **Stable Gradient Flow**: By maintaining normalized inputs, gradients don't undergo extreme changes in magnitude, leading to more stable and consistent learning.\n",
    "\n",
    "3. **Higher Learning Rates**: Batch Normalization enables the use of higher learning rates, speeding up training and improving model performance.\n",
    "\n",
    "4. **Regularization Effect**: Batch Normalization introduces a slight amount of noise to the inputs in each mini-batch, acting as a form of regularization and reducing overfitting.\n",
    "\n",
    "5. **Reduced Dependency on Initialization**: Batch Normalization reduces the sensitivity of a model to the choice of weight initialization, making network training more robust.\n",
    "\n",
    "## Working Principle of Batch Normalization\n",
    "\n",
    "Batch Normalization operates within each layer of the neural network. Here's how it works:\n",
    "\n",
    "1. **Normalization Step**: For each mini-batch during training, Batch Normalization normalizes the inputs of a layer by subtracting the mini-batch mean and dividing by the mini-batch standard deviation. This centers the inputs around zero and scales them to have unit variance.\n",
    "\n",
    "2. **Scaling and Shifting**: After normalization, the normalized inputs are scaled by a learnable parameter (gamma) and shifted by another learnable parameter (beta). This step allows the model to adapt to the most appropriate scale and mean for each layer.\n",
    "\n",
    "3. **Learnable Parameters**: Gamma and beta are learnable parameters that the network adjusts during training through backpropagation. This enables the network to decide whether to use the normalized values or revert to the original ones if it's more beneficial for a particular task.\n",
    "\n",
    "4. **Inference**: During inference (when making predictions), the normalization is applied similarly to the training process. However, instead of using mini-batch statistics, population statistics (mean and variance over the entire training dataset) are typically used to ensure consistency.\n",
    "\n",
    "In summary, Batch Normalization helps neural networks converge faster and generalize better by normalizing and stabilizing inputs at each layer. The learnable scaling and shifting parameters allow the network to adapt the normalization to the specific requirements of the task at hand. This technique has become a standard practice in deep learning architectures, contributing to improved training dynamics and higher model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 19:26:13.125312: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-16 19:26:13.907286: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-16 19:26:13.909053: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 19:26:19.083165: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109386 (427.29 KB)\n",
      "Trainable params: 109386 (427.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 19:26:26.396631: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/keras/src/backend.py:5714: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2390 - accuracy: 0.9290 - val_loss: 0.1283 - val_accuracy: 0.9618\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1038 - accuracy: 0.9682 - val_loss: 0.0916 - val_accuracy: 0.9705\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0714 - accuracy: 0.9779 - val_loss: 0.1027 - val_accuracy: 0.9684\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0564 - accuracy: 0.9825 - val_loss: 0.0838 - val_accuracy: 0.9738\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0437 - accuracy: 0.9853 - val_loss: 0.0821 - val_accuracy: 0.9766\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0336 - accuracy: 0.9893 - val_loss: 0.1076 - val_accuracy: 0.9700\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0276 - accuracy: 0.9911 - val_loss: 0.0937 - val_accuracy: 0.9758\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0248 - accuracy: 0.9919 - val_loss: 0.0891 - val_accuracy: 0.9764\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0203 - accuracy: 0.9934 - val_loss: 0.0985 - val_accuracy: 0.9775\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0188 - accuracy: 0.9934 - val_loss: 0.1075 - val_accuracy: 0.9758\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 10)                40        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109426 (427.45 KB)\n",
      "Trainable params: 109406 (427.37 KB)\n",
      "Non-trainable params: 20 (80.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Implementation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Flatten the images and create the neural network\n",
    "def build_model(use_batch_norm=False):\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compile the model\n",
    "def compile_model(model):\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Without Batch Normalization\n",
    "model_without_bn = build_model(use_batch_norm=False)\n",
    "compile_model(model_without_bn)\n",
    "model_without_bn.summary()\n",
    "\n",
    "# Train the model without batch normalization\n",
    "history_without_bn = model_without_bn.fit(train_images, train_labels,\n",
    "                                          epochs=10,\n",
    "                                          validation_data=(test_images, test_labels))\n",
    "\n",
    "# With Batch Normalization\n",
    "model_with_bn = build_model(use_batch_norm=True)\n",
    "compile_model(model_with_bn)\n",
    "model_with_bn.summary()\n",
    "\n",
    "# Train the model with batch normalization\n",
    "history_with_bn = model_with_bn.fit(train_images, train_labels,\n",
    "                                    epochs=10,\n",
    "                                    validation_data=(test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment and Analysis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Different batch sizes to experiment with\n",
    "batch_sizes = [32, 128, 512]\n",
    "\n",
    "# Initialize lists to store training histories\n",
    "histories = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    model = build_model(use_batch_norm=True)\n",
    "    compile_model(model)\n",
    "    \n",
    "    # Train the model with different batch sizes\n",
    "    history = model.fit(train_images, train_labels,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=10,\n",
    "                        validation_data=(test_images, test_labels))\n",
    "    histories.append(history)\n",
    "    \n",
    "# Plot training and validation accuracy for different batch sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, history in enumerate(histories):\n",
    "    plt.plot(history.history['val_accuracy'], label=f'Batch Size {batch_sizes[i]}')\n",
    "plt.title('Validation Accuracy with Different Batch Sizes')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
